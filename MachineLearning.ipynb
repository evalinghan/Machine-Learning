{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/evascheller/anaconda3/envs/CS155/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import Lasso, RidgeCV, LassoCV,LogisticRegressionCV, ElasticNet\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import tensorflow as tf \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from sklearn.decomposition import PCA\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier, XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions that I will usually use for a ML project\n",
    "def load_data(filename, skiprows = 1):\n",
    "    \"\"\"\n",
    "    Function loads data stored in the file filename and returns it as a numpy ndarray.\n",
    "    \n",
    "    Inputs:\n",
    "        filename: given as a string.\n",
    "        \n",
    "    Outputs:\n",
    "        Data contained in the file, returned as a numpy ndarray\n",
    "    \"\"\"\n",
    "    return np.loadtxt(filename, skiprows=skiprows, delimiter=',')\n",
    "def write_file(filename, ID, y_test):\n",
    "    '''\n",
    "    Function to write a file of predicted results given a filename, ID-numbers, and predicted y-values for test data\n",
    "    '''\n",
    "    import csv\n",
    "    with open(filename,'w') as f:\n",
    "        f.write('id,actual_wait div 60000\\n')\n",
    "        writer=csv.writer(f,delimiter=',')\n",
    "        writer.writerows(zip(ID,list((y_test))))\n",
    "    f.close()\n",
    "    \n",
    "def Dimensionality_reduction_PCA(x_train, dimensions):\n",
    "    '''\n",
    "    This function performs PCA on training data and returns the\n",
    "    reduced dimensionality array.\n",
    "    Inputs: \n",
    "        x_train: the training data input\n",
    "        dimensions: the number of wanted dimensions\n",
    "    Output: \n",
    "        x_train_reduced: the array with reduced dimensionsionality to \n",
    "        the number given by the dimensions paramter\n",
    "    '''\n",
    "    pca = PCA(n_components = dimensions)\n",
    "    x_train_reduced = pca.fit_transform(x_train) #fit and transform the training input data\n",
    "    return x_train_reduced\n",
    "\n",
    "def data_reduction(x_train, percentage_threshold):\n",
    "    '''\n",
    "    This function performs PCA on training data and returns the\n",
    "    reduced dimensionality array.\n",
    "    Inputs: \n",
    "        x_train: the training data input\n",
    "        dimensions: the number of wanted dimensions\n",
    "    Output: \n",
    "        x_train_reduced: the array with reduced dimensionsionality to \n",
    "        the number given by the dimensions paramter\n",
    "    '''\n",
    "    shape = x_train.shape\n",
    "    delete_cols = [] # list to hold columns to delete\n",
    "\n",
    "    for i in range(shape[1]):\n",
    "        col = x_train[:,i]\n",
    "        unique, counts = np.unique(col, return_counts=True)\n",
    "        maxPercent = np.max(counts) / shape[0]\n",
    "        if(maxPercent > percentage_threshold): # if the percentage of a certain class is high enough, then slice.\n",
    "            delete_cols.append(i)\n",
    "    return delete_cols\n",
    "\n",
    "def delete_cols(dataset, delete_cols):\n",
    "    '''\n",
    "    Function to delete columns returned by the data_reduction function\n",
    "    '''\n",
    "    return np.delete(dataset, delete_cols, 1)\n",
    "\n",
    "def normalize_data(x_data):\n",
    "    '''\n",
    "    Function to normalize data. \n",
    "    '''\n",
    "    new_x = x_data.copy()\n",
    "    shape = new_x.shape\n",
    "    for i in range(shape[1]):\n",
    "        col = new_x[:,i]\n",
    "        maxVal = np.max(col)\n",
    "        new_x[:,i] /= maxVal\n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load training data\n",
    "X_train = load_data('Data/X_train.csv')\n",
    "X_test = load_data('Data/X_test.csv')\n",
    "y_train = load_data('Data/y_train.csv')\n",
    "\n",
    "#The optimized PCA dimensions were found qualitatively through a loop of simulations and comparing cross validation scores\n",
    "x_train_PCA_reduced = Dimensionality_reduction_PCA(X_train, 3) #Reduce data dimensions\n",
    "x_test_PCA_reduced = Dimensionality_reduction_PCA(X_test, 3) #Reduce data dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale data to use in certain regression models below\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_ST = scaler.transform(X_train)\n",
    "X_test_ST = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evascheller/anaconda3/envs/CS155/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/evascheller/anaconda3/envs/CS155/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/evascheller/anaconda3/envs/CS155/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/evascheller/anaconda3/envs/CS155/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/evascheller/anaconda3/envs/CS155/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4326155209872621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evascheller/anaconda3/envs/CS155/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LassoCV(alphas=[0.0001, 0.001, 0.01, 0.1, 1, 2, 3, 5, 10, 100], copy_X=True,\n",
       "    cv=5, eps=0.001, fit_intercept=True, max_iter=1000, n_alphas=100,\n",
       "    n_jobs=None, normalize=False, positive=False, precompute='auto',\n",
       "    random_state=None, selection='cyclic', tol=0.0001, verbose=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's try to do a simple approach with single regressions\n",
    "#Starting with lasso regression\n",
    "lasso1 = LassoCV(alphas=[1e-4, 1e-3, 1e-2, 1e-1, 1, 2, 3, 5, 10,100], cv=5)\n",
    "#All training data scores were evaluated using 5-fold cross-validation\n",
    "#Parameters were optimized by running cross validation for multiple parameter combinations in each cell\n",
    "CV_score_lasso = np.mean(cross_val_score(lasso1,X_train,y_train,cv=5))\n",
    "print(np.mean(CV_score_lasso))\n",
    "lasso1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4312441040980386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 2.e+00, 3.e+00, 5.e+00,\n",
       "       1.e+01, 1.e+02]),\n",
       "    cv=5, fit_intercept=True, gcv_mode=None, normalize=False, scoring=None,\n",
       "    store_cv_values=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Next ridge regression\n",
    "ridge1 = RidgeCV(alphas=[1e-4, 1e-3, 1e-2, 1e-1, 1, 2, 3, 5, 10,100], cv=5)\n",
    "#All training data scores were evaluated using 5-fold cross-validation\n",
    "#Parameters were optimized by running cross validation for multiple parameter combinations in each cell\n",
    "CV_score_ridge = np.mean(cross_val_score(ridge1,X_train_ST,y_train,cv=5))\n",
    "print(CV_score_ridge)\n",
    "ridge1.fit(X_train_ST,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3596119916794528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=1.6, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
       "      max_iter=10000, normalize=False, positive=False, precompute=False,\n",
       "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hmm not great, let's try a regularized expression with elastic net\n",
    "EN = ElasticNet()\n",
    "EN.set_params(alpha=1.6,l1_ratio=0.5,max_iter=10000)\n",
    "#All training data scores were evaluated using 5-fold cross-validation\n",
    "#Parameters were optimized by running cross validation for multiple parameter combinations in each cell\n",
    "CV_score_EN = np.mean(cross_val_score(EN,X_train_ST,y_train,cv=5))\n",
    "print(CV_score_EN)\n",
    "EN.fit(X_train_ST,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6140667056988848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(bagging_fraction=0.9, bagging_freq=1, bagging_seed=1,\n",
       "       boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "       feature_fraction=0.5, feature_fraction_seed=1,\n",
       "       importance_type='split', learning_rate=0.05, max_bin=100,\n",
       "       max_depth=-1, min_child_samples=20, min_child_weight=0.001,\n",
       "       min_data_in_leaf=10, min_split_gain=0.0, min_sum_hessian_in_leaf=11,\n",
       "       n_estimators=100, n_jobs=-1, num_leaves=50, objective='regression',\n",
       "       random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's try gradient boosting with the LightGBM module\n",
    "params = {'objective': 'regression', 'num_leaves': 50,\n",
    "          'learning_rate': 0.05, 'n_estimators': 100,\n",
    "          'max_bin': 100, 'bagging_fraction': 0.9,\n",
    "          'bagging_freq': 1, 'feature_fraction': 0.5,\n",
    "          'feature_fraction_seed': 1, 'bagging_seed': 1,\n",
    "          'min_data_in_leaf': 10, 'min_sum_hessian_in_leaf': 11}\n",
    "lgbm1 = lgb.LGBMRegressor(**params)\n",
    "#All training data scores were evaluated using 5-fold cross-validation\n",
    "#Parameters were optimized by running cross validation for multiple parameter combinations in each cell\n",
    "CV_score_lgbm = np.mean(cross_val_score(lgbm1,X_train,y_train,cv=5))\n",
    "print(CV_score_lgbm)\n",
    "lgbm1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5925532268918335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.05, loss='huber', max_depth=4,\n",
       "             max_features='sqrt', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=10,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "             n_iter_no_change=None, presort='auto', random_state=5,\n",
       "             subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Okay, gradient boosting seems to work well\n",
    "#Let's try another method\n",
    "GBoost = GradientBoostingRegressor(n_estimators=500, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=1, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "#All training data scores were evaluated using 5-fold cross-validation\n",
    "#Parameters were optimized by running cross validation for multiple parameter combinations in each cell\n",
    "CV_score_GBoost = np.mean(cross_val_score(GBoost,X_train,y_train,cv=5))\n",
    "print(CV_score_GBoost)\n",
    "GBoost.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.583454248726604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=1, learning_rate=0.5, max_delta_step=0,\n",
       "       max_depth=6, min_child_weight=5, missing=None, n_estimators=10,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=1, silent=True,\n",
       "       subsample=0.8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Okay, let's try another gradient boosting method\n",
    "xgb1 = XGBRegressor(learning_rate=0.5,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                objective = 'reg:linear',\n",
    "                seed=1,\n",
    "                gamma=1,\n",
    "                max_depth=6,\n",
    "                min_child_weight=5,\n",
    "                n_estimators=10)\n",
    "#All training data scores were evaluated using 5-fold cross-validation\n",
    "#Parameters were optimized by running cross validation for multiple parameter combinations in each cell\n",
    "CV_score_XGB = np.mean(cross_val_score(xgb1,X_train,y_train,cv=5))\n",
    "print(np.mean(CV_score_XGB))\n",
    "xgb1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3975861680060328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdaBoostRegressor(base_estimator=None, learning_rate=0.05, loss='square',\n",
       "         n_estimators=10, random_state=5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ok, let's see what Adaboost can do in comparison to gradient boosting\n",
    "ABoost = AdaBoostRegressor(n_estimators=10,learning_rate=0.05,loss='square', random_state =5)\n",
    "#All training data scores were evaluated using 5-fold cross-validation\n",
    "#Parameters were optimized by running cross validation for multiple parameter combinations in each cell\n",
    "CV_score_ABoost = np.mean(cross_val_score(ABoost,X_train,y_train,cv=5))\n",
    "print(CV_score_ABoost)\n",
    "ABoost.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I found that the decision tree model below did not work well with data\n",
    "#DFmodel1 = DecisionTreeRegressor(criterion='mse')\n",
    "#DFmodel1.set_params(min_samples_leaf=20,max_depth=110,max_features='log2')\n",
    "#CV_score_DF = np.mean(cross_val_score(DFmodel1,X_train_ST,y_train))\n",
    "#print(CV_score_DF)\n",
    "#DFmodel1.fit(X_train_ST,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.57699988399546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=110,\n",
       "           max_features='log2', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=20, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since the decision tree model did not work well - let's train a couple random forests\n",
    "RFmodel1 = RandomForestRegressor(criterion='mse')\n",
    "RFmodel1.set_params(n_estimators=1000,min_samples_leaf=20,max_depth=110,max_features='log2')\n",
    "#All training data scores were evaluated using 5-fold cross-validation\n",
    "#Parameters were optimized by running cross validation for multiple parameter combinations in each cell\n",
    "CV_score_RF1 = np.mean(cross_val_score(RFmodel1,X_train,y_train,cv=5))\n",
    "print(CV_score_RF1)\n",
    "RFmodel1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=110,\n",
       "           max_features='log2', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=20, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=1500, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I found that the first random forest model did better than the second\n",
    "#RFmodel2 = RandomForestRegressor(criterion='mse')\n",
    "#RFmodel2.set_params(n_estimators=1500,min_samples_leaf=20,max_depth=110,max_features='log2')\n",
    "#CV_score_RF2 = np.mean(cross_val_score(RFmodel2,X_train,y_train,cv=5))\n",
    "#print(CV_score_RF2)\n",
    "#RFmodel2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Okay none of the models really did that well and I was running out of time\n",
    "#I tried to remove repetitive data columns using the helper functions above but it did not help the trainnig scores \n",
    "\n",
    "#I decided to stack my models\n",
    "#Here I'm stacking my test data prediction from all of my models\n",
    "lasso_pred = lasso1.predict(X_test)\n",
    "ridge_pred = ridge1.predict(X_test_ST)\n",
    "EN_pred = EN.predict(X_test_ST)\n",
    "LGBM_pred = lgbm1.predict(X_test)\n",
    "GBoost_pred = GBoost.predict(X_test)\n",
    "XGB_pred = xgb1.predict(X_test)\n",
    "ABoost_pred = ABoost.predict(X_test)\n",
    "RF1_pred = RFmodel1.predict(X_test)\n",
    "X_pred_stack = np.stack((lasso_pred,ridge_pred,ABoost_pred,RF1_pred,GBoost_pred,XGB_pred,EN_pred), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evascheller/anaconda3/envs/CS155/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n"
     ]
    }
   ],
   "source": [
    "#Here I'm stacking my training data predictions from all of my models\n",
    "RF1_pred = np.empty_like(y_train)\n",
    "GBoost_pred = np.empty_like(y_train)\n",
    "ABoost_train = np.empty_like(y_train)\n",
    "Lasso_pred = np.empty_like(y_train)\n",
    "Ridge_pred = np.empty_like(y_train)\n",
    "LGBM_train = np.empty_like(y_train)\n",
    "DF_train = np.empty_like(y_train)\n",
    "XGB_train = np.empty_like(y_train)\n",
    "EN_train = np.empty_like(y_train)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "i = 0\n",
    "for train, val in cv.split(X_train, y_train):\n",
    "    i += 1\n",
    "    print('Fold %d' % i)\n",
    "    RF1_pred[val] = RFmodel1.predict(X_train[val])\n",
    "    GBoost_pred[val] = GBoost.predict(X_train[val])\n",
    "    ABoost_train[val] = ABoost.predict(X_train_ST[val])\n",
    "    Lasso_pred[val] = lasso1.predict(X_train[val])\n",
    "    Ridge_pred[val] = ridge1.predict(X_train_ST[val])\n",
    "    LGBM_train[val] = lgbm1.predict(X_train[val])\n",
    "    XGB_train[val] = xgb1.predict(X_train[val])\n",
    "    EN_train[val] = EN.predict(X_train_ST[val])\n",
    "    \n",
    "X_train_stack = np.stack((Lasso_pred,Ridge_pred,ABoost_train,RF1_pred,GBoost_pred,XGB_train,EN_train), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I transform my stacked data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_stack)\n",
    "X_train_stack_ST = scaler.transform(X_train_stack)\n",
    "X_test_stack_ST = scaler.transform(X_pred_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7339713947782142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([1.0e-04, 1.0e-03, 1.0e-02, 1.0e-01, 5.0e-01, 1.0e+00, 1.5e+00,\n",
       "       2.0e+00, 3.0e+00, 5.0e+00, 1.0e+01, 3.0e+01, 5.0e+01, 1.0e+02]),\n",
       "    cv=5, fit_intercept=True, gcv_mode=None, normalize=False, scoring=None,\n",
       "    store_cv_values=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I now try different metamodel approaches with my stacked models\n",
    "#First is a ridge model\n",
    "ridge_meta = RidgeCV(alphas=[1e-4, 1e-3, 1e-2, 1e-1, 5e-1, 1, 1.5, 2, 3, 5, 10, 30, 50, 100], cv=5)\n",
    "CV_score_final = np.mean(cross_val_score(ridge_meta,X_train_stack_ST,y_train,cv=5))\n",
    "print(np.mean(CV_score_final))\n",
    "ridge_meta.fit(X_train_stack_ST, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evascheller/anaconda3/envs/CS155/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/evascheller/anaconda3/envs/CS155/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/evascheller/anaconda3/envs/CS155/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/evascheller/anaconda3/envs/CS155/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/evascheller/anaconda3/envs/CS155/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7338677404377764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evascheller/anaconda3/envs/CS155/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=0.05, copy_X=True, fit_intercept=True, l1_ratio=0.9,\n",
       "      max_iter=100, normalize=False, positive=False, precompute=False,\n",
       "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ok, I realize that my model did much better when stacked\n",
    "#I now try another metamodel approach with an elastic net\n",
    "#It did slightly better\n",
    "EN_meta = ElasticNet()\n",
    "EN_meta.set_params(alpha=0.05,l1_ratio=0.9,max_iter=100)\n",
    "CV_score_EN_meta = np.mean(cross_val_score(EN_meta,X_train_stack_ST,y_train,cv=5))\n",
    "print(np.mean(CV_score_EN_meta))\n",
    "EN_meta.fit(X_train_stack_ST,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               4000      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 525)               263025    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 525)               0         \n",
      "=================================================================\n",
      "Total params: 768,025\n",
      "Trainable params: 768,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "56490/56490 [==============================] - 110s 2ms/step - loss: 4.5041 - acc: 0.1126\n",
      "Epoch 2/10\n",
      "56490/56490 [==============================] - 25s 450us/step - loss: 4.2448 - acc: 0.1161\n",
      "Epoch 3/10\n",
      "56490/56490 [==============================] - 14s 256us/step - loss: 4.2090 - acc: 0.1179\n",
      "Epoch 4/10\n",
      "56490/56490 [==============================] - 10s 185us/step - loss: 4.1808 - acc: 0.1190\n",
      "Epoch 5/10\n",
      "56490/56490 [==============================] - 11s 195us/step - loss: 4.1579 - acc: 0.1198\n",
      "Epoch 6/10\n",
      "56490/56490 [==============================] - 11s 191us/step - loss: 4.1298 - acc: 0.1207\n",
      "Epoch 7/10\n",
      "56490/56490 [==============================] - 12s 207us/step - loss: 4.1084 - acc: 0.1230\n",
      "Epoch 8/10\n",
      "56490/56490 [==============================] - 15s 263us/step - loss: 4.0816 - acc: 0.1237\n",
      "Epoch 9/10\n",
      "56490/56490 [==============================] - 14s 248us/step - loss: 4.0616 - acc: 0.1249\n",
      "Epoch 10/10\n",
      "56490/56490 [==============================] - 14s 250us/step - loss: 4.0405 - acc: 0.1268\n"
     ]
    }
   ],
   "source": [
    "#Last I try to parse the stacked models into a NN\n",
    "#I try for a while to optimize parameters and number of layers, but the NN is not doing too well on the accuracy score\n",
    "#I had previously tried using NN to model data, and likewise it did not perform well on data, so I abandon this approach\n",
    "y_train_onehotencoded = to_categorical(y_train)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=X_train_stack.shape[1]))\n",
    "model.add(Activation('relu'))\n",
    "          \n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(525))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_stack, y_train_onehotencoded, batch_size=64, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Out of all my models the ridge meta model of my stacked models worked the best, so I chose this for my prediction\n",
    "y_test = ridge_meta.predict(X_test_stack_ST)\n",
    "\n",
    "#I write out my predictions to a file\n",
    "ID = []\n",
    "for i in range(len(y_test)):\n",
    "    ID.append(i)\n",
    "write_file('Data/y_test.csv',ID,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
